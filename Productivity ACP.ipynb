{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTNN1PRApG3DEFvO1TVpI6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install tensorflow==2.11 keras==2.11 transformers==4.27.4 nltk seaborn scikit-learn nltk emoji numpy pandas matplotlib"],"metadata":{"id":"lURXY9nVAR2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('omw-1.4')"],"metadata":{"id":"Pfh7O4EKASos"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('all')"],"metadata":{"id":"JkOtvbaaAXgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LapLig9FT7Sl"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import seaborn as sns\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import os\n","from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import re\n","import emoji\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from nltk.tokenize import word_tokenize\n","import pickle  # For saving models in Pickle format\n","\n","# Set random seeds for reproducibility\n","seed_value = 42\n","np.random.seed(seed_value)\n","tf.random.set_seed(seed_value)\n","\n","# Output directory for saving plots and model\n","output_dir = 'acp_models/AWARE_Productivityfinalresult'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Load your dataset and filter for 'productivity' domain\n","print(\"Loading dataset...\")\n","df = pd.read_csv(\"AWARE_Comprehensive.csv\")\n","df = df[df['domain'] == 'productivity']\n","print(f\"Dataset loaded: {len(df)} samples in 'productivity' domain.\")\n","\n","# Encode categorical labels\n","label_encoder = LabelEncoder()\n","df['encoded_label'] = label_encoder.fit_transform(df['sentiment'])\n","\n","# Initialize the BERT tokenizer and set max sequence length\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","max_length = 256\n","\n","# Initialize the BERT-based model\n","num_labels = len(df['encoded_label'].unique())\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n","\n","# Define NLTK functions\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'http\\S+', '', text)\n","    text = emoji.demojize(text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    tokens = word_tokenize(text)\n","    tokens = [word for word in tokens if word not in stop_words]\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    tokens = [stemmer.stem(word) for word in tokens]\n","    return ' '.join(tokens)\n","\n","def preprocess_data(df, tokenizer, max_length=256):\n","    texts = df['sentence'].apply(clean_text).tolist()\n","    labels = df['encoded_label'].tolist()\n","    tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n","    return tokenized, labels\n","\n","# Define stratified K-fold cross-validation\n","n_splits = 10\n","all_test_labels = []\n","all_predicted_labels = []\n","all_train_loss = []\n","all_train_accuracy = []\n","all_val_loss = []\n","all_val_accuracy = []\n","\n","for fold, (train_indices, val_indices) in enumerate(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(df['sentence'], df['encoded_label'])):\n","    print(f\"Fold {fold + 1}...\")\n","    fold_train_df = df.iloc[train_indices]\n","    fold_val_df = df.iloc[val_indices]\n","    train_df, val_df = train_test_split(fold_train_df, test_size=0.1, random_state=42)\n","\n","    train_data, train_labels = preprocess_data(train_df, tokenizer, max_length)\n","    val_data, val_labels = preprocess_data(val_df, tokenizer, max_length)\n","    test_data, test_labels = preprocess_data(fold_val_df, tokenizer, max_length)\n","\n","    train_data = {key: np.array(val) for key, val in train_data.items()}\n","    val_data = {key: np.array(val) for key, val in val_data.items()}\n","    test_data = {key: np.array(val) for key, val in test_data.items()}\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n","    model_checkpoint = ModelCheckpoint(os.path.join(output_dir, f'best_model_fold_{fold}'), save_best_only=True)\n","\n","    model.fit(\n","        train_data, np.array(train_labels),\n","        validation_data=(val_data, np.array(val_labels)),\n","        epochs=16, batch_size=8,\n","        callbacks=[early_stopping, model_checkpoint],\n","        verbose=2\n","    )\n","\n","    test_predictions = model.predict(test_data)['logits']\n","    test_predicted_labels = np.argmax(test_predictions, axis=1)\n","    all_test_labels.extend(test_labels)\n","    all_predicted_labels.extend(test_predicted_labels)\n","\n","# Print classification report\n","class_report = classification_report(all_test_labels, all_predicted_labels, target_names=label_encoder.classes_)\n","print(\"AWARE_Productivity: Classification Report Across Folds:\\n\", class_report)\n","\n","# Save the trained model in .h5 format\n","h5_path = os.path.join(output_dir, \"bert_model_weights.h5\")\n","model.save_weights(h5_path)\n","print(f\"Model weights saved as .h5 at: {h5_path}\")\n","\n","# Save the entire model using Pickle\n","pickle_path = os.path.join(output_dir, \"bert_model.pkl\")\n","with open(pickle_path, \"wb\") as f:\n","    pickle.dump(model, f)\n","print(f\"Model saved as Pickle at: {pickle_path}\")\n","\n","# Save the model in HuggingFace format\n","model.save_pretrained(output_dir)\n","print(f\"Model saved in HuggingFace format at: {output_dir}\")\n","\n","# Calculate the confusion matrix\n","conf_mat = confusion_matrix(all_test_labels, all_predicted_labels)\n","\n","# Visualize the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Productivity Domain: Confusion Matrix')\n","plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n","plt.show()\n","\n","# Final Message\n","print(\"Model training, evaluation, and saving completed for the 'productivity' domain.\")\n"]}]}